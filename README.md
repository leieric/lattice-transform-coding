# lattice-transform-coding

This is the official code for the ICLR 2025 paper [Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding](https://openreview.net/forum?id=Tv36j85SqR).

## Installation

In a python 3.11 environment, run
```
pip install -r requirements.txt
```

## Data
The synthetic sources are generated in [LTC/data.py](LTC/data.py). For the Physics source:
```
wget https://github.com/mandt-lab/RD-sandwich/raw/refs/heads/master/data/physics/ppzee-split=train.npy -P data/physics
wget https://github.com/mandt-lab/RD-sandwich/raw/refs/heads/master/data/physics/ppzee-split=test.npy -P data/physics
```

For the Speech source, follow this [script](https://github.com/mandt-lab/RD-sandwich/raw/refs/heads/master/data/speech/create_data.py) to generate the ``stft-split=train.npy`` and ``stft-split=test.npy`` files. Put these in ``data/speech``. 

## Running experiments
The implementation of Lattice Transform Coding (LTC) is contained in [LTC](LTC):
- All lattice quantizers are implemented in [LTC/quantizers.py](LTC/quantizers.py)
- Entropy models are implemented in [LTC/entropy_models.py](LTC/entropy_models.py)
- The LTC and BLTC architectures are implemented in [LTC/layers.py](LTC/layers.py)
- The main entry point for rate-distortion training is [train_model.py](train_model.py)

The scripts to launch training for all sources are contained in [scripts/](scripts/). 

To visualize the quantizer regions generated by a 2-d model, see [vis_quantizers.ipynb](vis_quantizers.ipynb).

## Citation
```
@inproceedings{
    lei2025approaching,
    title={Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding},
    author={Eric Lei and Hamed Hassani and Shirin Saeedi Bidokhti},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=Tv36j85SqR}
}
```